{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DCGAN 성능 평가를 위한 Simple CNN\n",
    "- Convolution 세팅 출처: https://cs230.stanford.edu/blog/handsigns/\n",
    "- Accuracy와 함께 confusion matrix를 그려 recall 값을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skin_dataset import SKIN_CANCER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에폭 수, learning rate, 배치 사이즈 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 로드하고, SubsetRandomSampler를 이용해 validation set(training set의 15%)도 설정해준다.\n",
    "- 데이터 크기를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_trn = SKIN_CANCER('skincancer', 'train')\n",
    "sc_tst = SKIN_CANCER('skincancer', 'test')\n",
    "trn_total_idx = list(range(len(sc_trn)))\n",
    "trn_total_idx = random.sample(trn_total_idx, len(trn_total_idx))\n",
    "trn_num = int(np.floor(0.85*len(sc_trn)))\n",
    "trn_idx, val_idx = trn_total_idx[:trn_num], trn_total_idx[trn_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_total_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(sc_trn, sampler=SubsetRandomSampler(trn_idx), batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(sc_trn, sampler=SubsetRandomSampler(val_idx), batch_size=batch_size, drop_last=True)\n",
    "tst_loader = DataLoader(sc_tst, batch_size=len(sc_tst), shuffle=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #we define convolutional layers \n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        #2 fully connected layers to transform the output of the convolution layers to the final output\n",
    "        self.fc1 = nn.Linear(in_features = 8*8*128, out_features = 128)\n",
    "        self.fcbn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 6)       \n",
    "        self.dropout_rate = 0.2\n",
    "        \n",
    "    def forward(self, s):\n",
    "        #we apply the convolution layers, followed by batch normalisation, \n",
    "        #maxpool and relu x 3\n",
    "        s = self.bn1(self.conv1(s))        # batch_size x 32 x 64 x 64\n",
    "        s = F.relu(F.max_pool2d(s, 2))     # batch_size x 32 x 32 x 32\n",
    "        s = self.bn2(self.conv2(s))        # batch_size x 64 x 32 x 32\n",
    "        s = F.relu(F.max_pool2d(s, 2))     # batch_size x 64 x 16 x 16\n",
    "        s = self.bn3(self.conv3(s))        # batch_size x 128 x 16 x 16\n",
    "        s = F.relu(F.max_pool2d(s, 2))     # batch_size x 128 x 8 x 8\n",
    "\n",
    "        #flatten the output for each image\n",
    "        s = s.view(-1, 8*8*128)  # batch_size x 8*8*128\n",
    "\n",
    "        #apply 2 fully connected layers with dropout\n",
    "        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))), \n",
    "        p=self.dropout_rate, training=self.training)    # batch_size x 128\n",
    "        s = self.fc2(s)                                     # batch_size x 6\n",
    "\n",
    "        return F.log_softmax(s, dim=1)\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss는 cross entropy loss를, optimizer는 Adam을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 30] trn loss: nan val loss: nan\n",
      "[2 / 30] trn loss: nan val loss: nan\n",
      "[3 / 30] trn loss: nan val loss: nan\n",
      "[4 / 30] trn loss: nan val loss: nan\n",
      "[5 / 30] trn loss: nan val loss: nan\n",
      "[6 / 30] trn loss: nan val loss: nan\n",
      "[7 / 30] trn loss: nan val loss: nan\n",
      "[8 / 30] trn loss: nan val loss: nan\n",
      "[9 / 30] trn loss: nan val loss: nan\n",
      "[10 / 30] trn loss: nan val loss: nan\n",
      "[11 / 30] trn loss: nan val loss: nan\n",
      "[12 / 30] trn loss: nan val loss: nan\n",
      "[13 / 30] trn loss: nan val loss: nan\n",
      "[14 / 30] trn loss: nan val loss: nan\n",
      "[15 / 30] trn loss: nan val loss: nan\n",
      "[16 / 30] trn loss: nan val loss: nan\n",
      "[17 / 30] trn loss: nan val loss: nan\n",
      "[18 / 30] trn loss: nan val loss: nan\n",
      "[19 / 30] trn loss: nan val loss: nan\n",
      "[20 / 30] trn loss: nan val loss: nan\n",
      "[21 / 30] trn loss: nan val loss: nan\n",
      "[22 / 30] trn loss: nan val loss: nan\n",
      "[23 / 30] trn loss: nan val loss: nan\n",
      "[24 / 30] trn loss: nan val loss: nan\n",
      "[25 / 30] trn loss: nan val loss: nan\n",
      "[26 / 30] trn loss: nan val loss: nan\n",
      "[27 / 30] trn loss: nan val loss: nan\n",
      "[28 / 30] trn loss: nan val loss: nan\n",
      "[29 / 30] trn loss: nan val loss: nan\n",
      "[30 / 30] trn loss: nan val loss: nan\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ztor7\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ztor7\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    trn_loss = []\n",
    "    val_loss = []\n",
    "    for i, data in enumerate(trn_loader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trn_loss.append(loss.item())\n",
    "    \n",
    "    net.eval()\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss.append(loss.item())\n",
    "    \n",
    "    print('[{} / {}] trn loss: {:.4f} val loss: {:.4f}'.format(epoch + 1, epochs, np.mean(trn_loss), np.mean(val_loss)))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
       "  (fcbn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_loss = []\n",
    "tst_acc = []\n",
    "net = net.to(torch.device('cpu'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.00 % Test Loss: 1.8159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ztor7\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(tst_loader, 0):\n",
    "    inputs, labels = data\n",
    "    output = net(inputs)\n",
    "    loss = criterion(output, labels)\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    acc = (preds == labels).sum() / len(output)\n",
    "    \n",
    "    tst_loss.append(loss.item())\n",
    "    tst_acc.append(acc.item())\n",
    "\n",
    "print('Test Accuracy: {:.2f} % Test Loss: {:.4f}'.format(np.mean(tst_acc)*100, np.mean(tst_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= confusion_matrix(labels, preds)\n",
    "cmdf= pd.DataFrame(cm, index=set(labels.numpy()), columns=set(labels.numpy()))\n",
    "cmdf = cmdf.rename(columns = {0: 'benign', 1: 'malignant'}, index={0: 'benign', 1: 'malignant'}, inplace = False); \n",
    "cmdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall 값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn, fp, fn, tp: (315, 45, 82, 218) recall:  0.7267\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp / (tp + fn)\n",
    "print('tn, fp, fn, tp:', (tn, fp, fn, tp), 'recall: ', np.round(recall, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
